---
title:"Are University towns have their mean housing prices less effected by recessions?"
date: 2019-10-22
tags: [Projects, Data Cleaning, Data Analysis]
excerpt: "Projects, Data Analysis, Data Science"
---
# Are University towns have their mean housing prices less effected by recessions?

# Data Acquisition

## Overview

<p style="text-align: justify;font-family: none;">This section describes data acquisition process for testing a hypothesis whether university towns have their housing prices less effected by recessions. This analysis requires a list of university towns, mean / median housing prices by city for the purpose of comparing housing prices in university towns versus non-university towns, and GDP growth for determining the timing of recessions.</p>

<p style="text-align: justify;font-family: none;">The next step, which includes cleaning and processing of the data, is described here.</p>

<p style="text-align: justify;font-family: none;">This project is based on assignments from Introduction to Data Science in Python by University of Michigan on Coursera</p>

<p style="text-align: justify;font-family: none;">The analysis for this project was performed in Python.</p>


## Data

<p style="text-align: justify;font-family: none;">All the data have been downloaded from the Coursera website. The original sources of the data and code for loading the data are described below.</p>
<p style="text-align: justify;font-family: none;">A list of university towns in the United States is available from the Wikipedia page on college towns. A university town is a city which has a high percentage of university students compared to the total population of the city. The following function imports a list of university towns and states in which these towns are located from a text file into a pandas data frame.</p>

```
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
import re
def get_university_data():
    university_towns_data = pd.read_csv('/Users/15832/Desktop/Travel/Data Science/New folder/Coursera Course/course1_downloads/course1_downloads/university_towns.txt', sep='/n',engine='python',header = None)
    df = university_towns_data
    df.columns = ["RegionName"]
    return df
get_university_data().head()
```

<p style="text-align: justify;font-family: none;">The first five rows of the resulting data frame are as follows:</p>

<img src="{{site.url}}{{site.baseurl}}/images/load_unv_data.PNG" alt="l">


<p style="text-align: justify;font-family: none;">The GDP over time of the United States in current dollars (the chained value in 2009 dollars in quarterly intervals) is from Bureau of Economic Analysis, US Department of Commerce, in the file gdplev.xlsx. A quarter is a specific three month period, Q1 is January through March, Q2 is April through June, Q3 is July through September, Q4 is October through December. GDP data used in the analysis are from the first quarter of 2000 onward. The following function imports the GDP data from an Excel file into a pandas data frame and drops the data that are not used in the analysis:</p>

```
def get_gdp_data():
    df = pd.read_excel('/Users/15832/Desktop/Travel/Data Science/New folder/Coursera Course/course1_downloads/course1_downloads/gdplev.xls', skiprows=5)
    GDP_Data = df.drop([0, 1]).reset_index(drop = True)
    GDP_Data= GDP_Data[['Unnamed: 4', 'GDP in billions of chained 2009 dollars.1']]
    GDP_Data.columns = ['Qaurters', 'GDP']
    GDP_Data = GDP_Data[GDP_Data.Qaurters.str.contains('20')].reset_index(drop = True)
    return GDP_Data
get_gdp_data().head()

```

<p style="text-align: justify;font-family: none;">The first five rows of the resulting data frame are as follows:</p>

<p style="text-align: justify;font-family: none;">Housing data for the United States is from the Zillow research data site. In particular, the datafile for all homes at a city level, City_Zhvi_AllHomes.csv, has median home sale prices at a fine grained level. The following function imports the housing data from a CSV file into a pandas data frame:</p>

```
def load_housing_data():
    city_housing_data = pd.read_csv('/Users/15832/Desktop/Travel/Data Science/New folder/Coursera Course/course1_downloads/course1_downloads/City_Zhvi_AllHomes.csv', skiprows=0)
    city_housing_data['State'] = city_housing_data['State'].replace({'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', 'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', 'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia', 'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', 'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam', 'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', 'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', 'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', 'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', 'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', 'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'})
    return city_housing_data
load_housing_data().head()

```    
   
<p style="text-align: justify;font-family: none;">The first five rows of the resulting data frame are as follows:</p>

Next step:[Data Preparation](/datapreparation/)

# Data Preparation

## Overview

<p style="text-align: justify;font-family: none;">This section describes clearing and processing of the data collected for testing a hypothesis whether university towns have their housing prices less effected by recessions.</p>

<p style="text-align: justify;font-family: none;">Data acquisition is described in the previous section.</p>

<p style="text-align: justify;font-family: none;">Analysis of the data is described in the next section.</p>

<p style="text-align: justify;font-family: none;">This project is based on assignments from Introduction to Data Science in Python by University of Michigan on Coursera</p>

<p style="text-align: justify;font-family: none;">The analysis for this project was performed in Python.</p>

<p style="text-align: justify;font-family: none;">Cleaning and Processing of the University Town Data</p>

<p style="text-align: justify;font-family: none;">The list of university towns includes entries of both university towns and states in which these towns are located in a single column. State names should be removed from that column and then added as the second column to a data frame with two columns corresponding to university towns and states they are in. The format of the DataFrame is: DataFrame( [ [“Michigan”, “Ann Arbor”], [“Michigan”, “Yipsilanti”] ], columns=[“State”, “RegionName”] )</p>

<p style="text-align: justify;font-family: none;">In addition to this transformation, certain characters and portions of the text need to be removed. Specifically:</p>

<p style="text-align: justify;font-family: none;">1.	For “State”, removing characters from “[” to the end.</p>
<p style="text-align: justify;font-family: none;">2.	For “RegionName”, when applicable, removing every character from either “[” or “ (“ or “:” to the end.</p>
<p style="text-align: justify;font-family: none;">3.	Finally, it is important to note that certain rows in the data represent universities and not towns. Those rows will be subsequently dropped when merging the university town data with housing price data.</p>
<p style="text-align: justify;font-family: none;">The following function performs the transformation described above, and then uses regular expressions to identify and remove the redundant text patterns.</p>

```
def get_list_of_university_towns():
        
    #Create two columns corresponding to university towns and states they are in 
    #and remove redundant data
    university_towns = get_university_data()
    university_towns['State']= university_towns[university_towns["RegionName"].str.contains('edit')]
    university_towns['State'] = (university_towns['State'].str.replace(r'\[.*','').fillna(method='ffill'))
    university_towns['RegionName'] = university_towns['RegionName'].str.replace(r'\W*\(.*','')
    university_towns = university_towns[['State', 'RegionName']]
    university_towns.drop_duplicates(keep=False, inplace=True)
    df1 = university_towns[university_towns.RegionName.str.contains('edit')].index
    university_towns = university_towns.drop(df1).reset_index(drop = True)
    return university_towns

get_list_of_university_towns().head() 

```  


<p style="text-align: justify;font-family: none;">The first five rows of the resulting data frame are as follows:</p>


## Processing of the GDP Data

<p style="text-align: justify;font-family: none;">Because subsequent analysis requires changes in GDP in order to determine the start, the end and the bottom of a recession, the following code creates leads and lags of GDP and then calculates changes in GDP:</p>

<p style="text-align: justify;font-family: none;">The first five rows of the resulting data frame are as follows:</p>

## Processing of the Housing Data

<p style="text-align: justify;font-family: none;">Finally, the monthly housing price data needs to be converted to quarters before it is analyzed along with quarterly GDP figures. The following function averages the monthly prices within each quarter. The resulting data frame has columns for 2000Q1 through 2016Q3, and a multi-index in the shape of [“State”, “RegionName”]. Then the function below merges housing data with state names using state codes. This step is necessary in order to subsequently merge the housing data with the university town data, which include state names but do not include state codes:</p>


```
def convert_housing_data_to_quarters():

    housing_data = load_housing_data()
    housing_data = pd.DataFrame(housing_data.drop(['Metro','CountyName','SizeRank'],  axis=1)).set_index(['RegionID', 'State','RegionName']).stack().reset_index()
    v =  housing_data.rename(columns={'level_3':'Year-Mo',0:'MedianPrice'})
    month_to_qtr = {'01':'q1','02':'q1','03':'q1','04':'q2','05':'q2','06':'q2',
                '07':'q3','08':'q3','09':'q3','10':'q4','11':'q4','12':'q4'}
    v['YearQtr'] = v['Year-Mo'].str[0:4] + (v['Year-Mo'].str[-2:].map(month_to_qtr))
    v = v[v['YearQtr']>='2000q1'].copy()
    v = (v.drop(['Year-Mo'], axis=1)
         .groupby(['RegionID', 'State','RegionName','YearQtr'])
         .mean()
         .unstack())
    v.index = v.index.droplevel()
    v.columns = v.columns.droplevel()
    del v.columns.name
    return v
convert_housing_data_to_quarters()
```
# Analysis

## Overview

<p style="text-align: justify;font-family: none;">This section tests a hypothesis whether university towns have their housing prices less effected by recessions, using a t-test. The test compares the ratio of the mean housing price in the quarter of the recession bottom to the quarter before the recession starts between university and non-university towns. (price_ratio = recession_bottom / quarter_before_recession).</p>
<p style="text-align: justify;font-family: none;">Data cleaning and processing are described in the previous section.</p>
<p style="text-align: justify;font-family: none;">This project is based on assignments from Introduction to Data Science in Python by University of Michigan on Coursera</p>
<p style="text-align: justify;font-family: none;">The analysis for this project was performed in Python.</p>

<p style="text-align: justify;font-family: none;">Finding Recession Start, End and Bottom</p>
<p style="text-align: justify;font-family: none;">In order to test the hypothesis whether university towns have their housing prices less effected by recessions, we first need to define what we mean by recession.</p>
<p style="text-align: justify;font-family: none;">A recession is defined as starting with two consecutive quarters of GDP decline, and ending with two consecutive quarters of GDP growth.</p>
<p style="text-align: justify;font-family: none;">A recession bottom is the quarter within a recession which had the lowest GDP.</p>
<p style="text-align: justify;font-family: none;">We then proceed by finding the recession start, recession end and recession bottom in the data. We will use these figures for calculating the ratios of the mean housing prices discussed above.</p>
<p style="text-align: justify;font-family: none;">The following function returns the year and quarter of the recession start time as a string value in a format such as 2005Q3:</p>

```
def get_recession_start():
    GDP_Data = get_gdp_data()
    GDP_Data['Change in GDP'] = GDP_Data['GDP'].diff().shift(-1)
    recission = GDP_Data.loc[GDP_Data['Change in GDP'].idxmin()]
    return recission['Qaurters']
get_recession_start()
```
<p style="text-align: justify;font-family: none;">The results show that the recession started in 2008Q3.</p>
<p style="text-align: justify;font-family: none;">The next function returns the year and quarter of the recession end time as a string value in a format such as 2005Q3:</p>
```
def get_recession_end():
    
    GDP_Data = get_gdp_data()
    GDP_Data['Change in GDP'] = GDP_Data['GDP'].diff().shift(-1)
    recission_years = GDP_Data[34:].reset_index(drop = True)
    recission_end = recission_years[(recission_years['GDP'] > recission_years['GDP'].shift(1))&
                (recission_years['GDP'].shift(1) > recission_years['GDP'].shift(2))].copy()
    return recission_end.iloc[0,0]
       
get_recession_end()
```


<p style="text-align: justify;font-family: none;">The next function returns the year and quarter of the recession bottom as a string value in a format such as 2005Q3:</p>

```
def get_recession_bottom():
    GDP_Data = get_gdp_data()
    GDP_Data['Change in GDP'] = GDP_Data['GDP'].diff().shift(-1)
    recission_years = GDP_Data[34:].reset_index(drop = True)
    recission_end = recission_years[(recission_years['GDP'] > recission_years['GDP'].shift(1))&
                (recission_years['GDP'].shift(1) > recission_years['GDP'].shift(2))].copy()
    recission_bottom = recission_years[0:5].reset_index(drop = True)
    recission_bottom_quarter = recission_bottom.loc[recission_bottom['GDP'].idxmin()]
    return recission_bottom_quarter['Qaurters']
get_recession_bottom()
```


## The t-test

<p style="text-align: justify;font-family: none;">First, we calculate the decline (or growth) in housing prices between the recession start and recession bottom. Then we run a t-test to determine whether such price declines in university towns are statistically significantly different from the declines in non-university towns. The t-test returns whether the null hypothesis (that the two groups are the same) is rejected as well as the p-value of the confidence.</p>
<p style="text-align: justify;font-family: none;">The following function returns a tuple (different, p, better) where “different” = True if the the p-value of the t-test p < 0.01 (we reject the null hypothesis), or “different” = False if otherwise (we cannot reject the null hypothesis). The value for “better” is either “university town” or “non-university town” depending on which has a higher mean price ratio (which is equivalent to a reduced market loss).</p>
```
from scipy.stats import ttest_ind, ttest_ind_from_stats
from scipy.special import stdtr
def run_ttest():
    
    university_towns = get_list_of_university_towns()
    university_towns = university_towns[['State', 'RegionName']]
    university_towns['University_Town'] = 1
    university_towns = university_towns.set_index(['State', 'RegionName'])
    
    new_housing_data = convert_housing_data_to_quarters()
    new_housing_data = new_housing_data.merge(university_towns, how = 'left', left_index = True, right_index = True)
    new_housing_data.University_Town[new_housing_data.University_Town.isnull()] = 0
       
    recession_start = get_recession_start()
    recession_bottom = get_recession_bottom()
    y = new_housing_data[[recession_start, recession_bottom, 'University_Town']]
    y['Change'] = y['2009q2'] / y['2008q3']
    
    ut = y[y['University_Town'] == 1]['Change'].dropna()
    not_ut = y[y['University_Town'] == 0]['Change'].dropna()
    t = ttest_ind(ut, not_ut)
    p = t[1]
     
    if p < 0.01:
        different = True    
    else:
        different = False
    
    if ut.mean() < not_ut.mean():
        better = 'non-university town'
    else:
        better = 'university town'
        
    return (different, p, better)
        
run_ttest()
```
## Results
<p style="text-align: justify;font-family: none;">The resulting tuple is (True, 0.0039, ‘university town’), which means that the null hypotheses (the two groups are the same) is rejected with the p-value of 0.0031. This result implies that university towns experienced smaller housing price declines during the recession of 2008-2009 compared to non-university towns.</p>

